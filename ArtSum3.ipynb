{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Text summarization using SPACY\n",
      ",A person is considered at high risk of having caught the infection if they have had household or sexual contact with someone with monkeypox, or have changed the bedding of an infected person without wearing personal protective equipment (PPE).,Symptoms, which include a high temperature, aches, and a rash of raised spots that later turn into blisters, are typically mild and for most people clear up within two to four weeks. ,\"However the likelihood of further spread of the virus through close contact for example during sexual activities amongst persons with multiple sexual partners is considered to be high\".\n",
      "\n",
      "\n",
      "length of original text: 2810\n",
      "length of autosummarization text using SPACY 615\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "## Taking text for summarization\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "url = \"https://www.bbc.com/news/health-61553822\"\n",
    "response = requests.get(url)\n",
    "#print(response)\n",
    "\n",
    "html = response.content\n",
    "#print(html)\n",
    "soup = bs(html,\"lxml\")\n",
    "\n",
    "import re\n",
    "text = \",\".join([el.text for el in soup.find_all(\"div\", [re.compile('.*RichTextContainer.*')], recursive=True)])\n",
    "#print(soup.find(property=\"articleBody\"))\n",
    "#article = [el.text for el in soup.find_all(\"div\", [re.compile('.*RichTextContainer.*')], recursive= True)]\n",
    "#print(text)\n",
    "\n",
    "\n",
    "# List of stop words\n",
    "stopwords = list( STOP_WORDS)\n",
    "# pass document into spacy and store in \"doc\" object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "#print(tokens)\n",
    "\n",
    "#List item\n",
    "# add \\n to the punchuvation list\n",
    "punctuation = punctuation + '\\n' + '\\n\\n'\n",
    "punctuation\n",
    "\n",
    "# Preparing a dictionary for word frequencies¶\n",
    "word_frequencies = {}                       # Dictionary Name\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():   \n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] +=1\n",
    "#print(word_frequencies)\n",
    "max_frequcncy = max(word_frequencies.values())\n",
    "\n",
    "# Normalize the word frencies\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequcncy\n",
    "     \n",
    "#sentence tokenization -Calculate sentences scores¶   By creating a Dictionay for sentences and its normalized frequencies\n",
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "#print(sentence_tokens)\n",
    "sentence_scores = {}                                            #create Dictionary\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():        \n",
    "            if sent not in sentence_scores.keys():              # add word normalized frequencies count in each of these sentences,then with maximum value we are going to find important sentence \n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "sentence_scores\n",
    "\n",
    "# Can tweek percentage\n",
    "# Get 30% of important sentences with maximum score - using nlargest\n",
    "from heapq import nlargest\n",
    "select_length = int(len(sentence_tokens)*0.1)\n",
    "select_length\n",
    "summary = nlargest(select_length , sentence_scores , key = sentence_scores.get)\n",
    "summary   # Got 4 important sentences\n",
    "final_summary = [word.text for word in summary]\n",
    "final_summary\n",
    "#print(len(final_summary))\n",
    "#combining above 4 lines togeather to from a summary\n",
    "summary = ' ' .join(final_summary)\n",
    "summary\n",
    "summary1 = summary.replace('\\n', ' ')\n",
    "print('\\n')\n",
    "print(\"Text summarization using SPACY\")\n",
    "print(summary1)\n",
    "print('\\n')\n",
    "print(\"length of original text:\",len(text))\n",
    "print(\"length of autosummarization text using SPACY\",len(summary))\n",
    "\n",
    "# Summarization using 'gensim'\n",
    "#from gensim.summarization import summarize\n",
    "#print('\\n')\n",
    "#print(\"Text summarization using gensim\")\n",
    "#summarize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('BBC Health data analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1663855e15dd368e085983d8036e05c4981c5eea6654526205ec9ed46155b6d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
